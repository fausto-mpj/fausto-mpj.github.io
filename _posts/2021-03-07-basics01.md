---
title: Basic Training 01
subtitle: Matrix Notation and Ordinary Least Squares
cover-img: "/assets/img/Matrix.jpg"
tags: [Basics, Least Squares]
comments: true
---

### The Problem

So, for the inaugural post, I want to write about something uncontroversial. Joking. We're delving in a wild territory where the very fabric of society dissolves into nothingness: **matrix notation**. First, I'd like to fabricate a pretty innacurate statistic.

> 9 out of 10 books with any Maths in them employ idiosyncratic matrix notations.

Given this (false) information and assuming that eventually we're going to mess with matrices, it seems reasonable enough to establish our ground rules and notation right off the bat. In order to do this, we're going to use *Linear Algebra and its Applications* (4th edition) by [Gilbert Strang](http://www-math.mit.edu/~gs/) as our style guide.

However, I don't want to talk solely about notation and I'm most certainly not ready to give any clear and concise explanation on matrices and vector spaces. This is why I chose a common application to function as a foreground: **Ordinary Least Squares** (OLS). Also known as "the thing you will do the most if you ever try Statistics". In fact, it is one of those things most (Stats) students do so many times that eventually they lose any sense of what is happening. Of course, this is where danger lives.

### Least Squares

The OLS is *mechanically* simple. Suppose we have a \\(n \times p\\) matrix \\(\\mathbf{X}\\), a \\(p \times 1\\) vector \\(\\mathbf{\beta}\\), and a \\(n \times 1\\) vector \\(\\mathbf{y}\\), where the \\(i\\)-eth column of \\(\\mathbf{X}\\) is denoted \\(\\mathbf{x}\_{i}\\).

$$
\mathbf{X} = \left(
\begin{array}{cccc}
x_{1,1} & x_{1,2} & ... & x_{1,p} \\
x_{2,1} & x_{2,2} & ... & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & ... & x_{n,p}
\end{array}
\right),
\quad
\mathbf{y} = \left(
\begin{array}{c}
y_{1} \\
y_{2} \\
... \\
y_{n}
\end{array}
\right),
\quad
\mathbf{\beta} = \left(
\begin{array}{c}
\beta_{1} \\
\beta_{2} \\
... \\
\beta_{p}
\end{array}
\right).
$$

In this setup, we can read each row of \\(\\mathbf{X}\\) as an observation of \\(p\\) variables \\((\\mathbf{x}\_{1}, ..., \\mathbf{x}\_{p})\\). For example, consider a \\(n \times 2\\) matrix \\(\\mathbf{X}\\) containing the information about the height and weight from a sample of a certain population. If \\(\\mathbf{x}\_{1}\\) is a variable concerning the height and \\(\\mathbf{x}\_{2}\\) is a variable concerning weight, then \\((x\_{1,1}, x\_{1,2})\\) is the first row of \\(\\mathbf{X}\\) and the height and weight of the first person that was measured in this sample.

Teste:

\\[
\\begin{bmatrix}
x & y \\\\
x\_{i} & y\_{i}
\\end{bmatrix}
\\]
