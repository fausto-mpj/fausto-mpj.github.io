---
title: Basic Training 01
subtitle: Matrix Notation and Ordinary Least Squares
cover-img: "/assets/img/Matrix.jpg"
tags: [Basics, Least Squares]
comments: true
---

### The Problem

So, for the inaugural post, I want to write about something uncontroversial. Joking. We're delving in a wild territory where the very fabric of society dissolves into nothingness: **matrix notation**. First, I'd like to fabricate a pretty innacurate statistic.

> 9 out of 10 books with any Maths in them employ idiosyncratic matrix notations.

Given this (false) information and assuming that eventually we're going to mess with matrices, it seems reasonable enough to establish our ground rules and notation right off the bat. In order to do this, we're going to use *Linear Algebra and its Applications* (4th edition) by [Gilbert Strang](http://www-math.mit.edu/~gs/) as our style guide.

However, I don't want to talk solely about notation and I'm most certainly not ready to give any clear and concise explanation on matrices and vector spaces. This is why I chose a common application to function as a foreground: **Ordinary Least Squares** (OLS). Also known as "the thing you will do the most if you ever try Statistics". In fact, it is one of those things most (Stats) students do so many times that eventually they lose any sense of what is happening. Of course, this is where danger lives.

### Least Squares

The OLS is *mechanically* simple. Suppose we have a \\( n \times p \\) *design matrix* \\( \mathbf{X} \\), a \\( p \times 1 \\) *coefficients' vector* \\( \mathbf{\beta} \\), and a \\( n \times 1 \\) *response vector* \\( \mathbf{y} \\), where the \\( i \\)-eth column of \\( \mathbf{X} \\) is denoted \\( \mathbf{x}\_{i} \\) and \\( \mathbf{x}\_{1} = \mathbf{1} \\). In this setup, we can read each row of \\( \mathbf{X} \\) as an observation of \\( p - 1 \\) *regressor variables* \\( (\mathbf{x}\_{2}, \mathbf{x}\_{3}, \ldots, \mathbf{x}\_{p}) \\) of a sample with size \\( n \\).

$$
\mathbf{X} = \left(
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \ldots & x_{1,p} \\
x_{2,1} & x_{2,2} & \ldots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \ldots & x_{n,p}
\end{array}
\right),
\quad
\mathbf{y} = \left(
\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{array}
\right),
\quad
\mathbf{\beta} = \left(
\begin{array}{c}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{p}
\end{array}
\right).
$$

We're interested in finding a *linear relationship* between \\( \mathbf{X} \\) and \\( \mathbf{y} \\). In other words, we want to know if we can use a [hyperplane](https://mathworld.wolfram.com/Hyperplane.html) to model accurately their relationship. Let us recall the equation of the hyperplane, which is a generalization of the well-known plane equation. Let \\( a_{1}, \ldots, a_{p}, c \in \mathbb{R} \\), with at least one \\( a_{i} \neq 0 \\). A hyperplane is defined by the set of all vectors \\( \mathbf{x} \in \mathbb{R}^{p} \\) such as

$$
a_{1} x_{1} + \ldots + a_{p} x_{p} = c .
$$

When working on bidimensional spaces, \\( p = 2 \\), this hyperplane is simply a line:

$$
a x_{1} + b x_{2} = c \quad \rightarrow \quad a + b x_{2} = c .
$$

With no loss of generalization, let us use the case in which \\( p = 2 \\) in order to visualize what is happening in OLS.
